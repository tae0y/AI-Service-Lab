{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90408646",
   "metadata": {},
   "source": [
    "## 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e7418",
   "metadata": {},
   "source": [
    "환경준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84db84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 호스트 머신에서 가상환경 구성\n",
    "#uv venv && source .venv/bin/activate\n",
    "#uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affcb6f",
   "metadata": {},
   "source": [
    "mlx 형태로 모델 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecb05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlx_lm 패키지를 사용하여 HF의 특정 모델을 지정된 경로/이름으로 mlx 모델로 변환\n",
    "!uv run python -m mlx_lm.convert \\\n",
    "    --hf-path Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "    --mlx-path models/qwen-0.5b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf9736",
   "metadata": {},
   "source": [
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8476524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13.9M  100 13.9M    0     0  27.0M      0 --:--:-- --:--:-- --:--:-- 27.0M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4778k  100 4778k    0     0  13.2M      0 --:--:-- --:--:-- --:--:-- 13.2M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 150000 examples [00:00, 540720.03 examples/s]\n",
      "Generating test split: 50000 examples [00:00, 511634.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 원본데이터를 직접 다운받아서\n",
    "!mkdir -p data\n",
    "!curl -L -o data/ratings_train.txt https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "!curl -L -o data/ratings_test.txt  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "\n",
    "# 데이터셋을 로드한다\n",
    "from datasets import load_dataset\n",
    "data_files = {\"train\":\"data/ratings_train.txt\", \"test\":\"data/ratings_test.txt\"}\n",
    "ds = load_dataset(\"csv\", data_files=data_files, sep=\"\\t\")  # 컬럼: id, document, label\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc5b5147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 150/150 [00:00<00:00, 1912.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 1949.10ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5341357"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파케 형태로 변환해둔다\n",
    "ds[\"train\"].to_parquet(\"data/nsmc-train.parquet\")\n",
    "ds[\"test\"].to_parquet(\"data/nsmc-test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93865ff",
   "metadata": {},
   "source": [
    "데이터는 다음과 같은 형태로 준비되어 있다\n",
    "- 데이터 ID\n",
    "- 영화감상평\n",
    "- 긍정/부정 라벨 (1이면 긍정, 0이면 부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62820f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df13c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "def make_chat(example):\n",
    "    txt = example[\"document\"]\n",
    "    lab = \"긍정\" if example[\"label\"]==1 else \"부정\"\n",
    "    # 간단한 지시문 프롬프트(분류 태스크를 대화형으로)\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":\"너는 한국어 감성분석 어시스턴트야.\"},\n",
    "        {\"role\":\"user\",\"content\":f\"다음 리뷰의 감성을 한 단어로만 답해줘(긍정/부정). 리뷰: {txt}\"},\n",
    "        {\"role\":\"assistant\",\"content\":lab}\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def dump(ds, path, k):\n",
    "    data = [make_chat(x) for x in ds.select(range(min(k, len(ds))))]\n",
    "    with open(path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for r in data: f.write(json.dumps(r, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "dump(ds[\"train\"], \"data/nsmc_train_10k.jsonl\", 10_000)\n",
    "dump(ds[\"test\"], \"data/nsmc_test_2k.jsonl\", 2_000)\n",
    "print(\"ok\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a93cf7",
   "metadata": {},
   "source": [
    "## LoRA 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52922c12",
   "metadata": {},
   "source": [
    "- **LoRA** (Low Rank Adaptation)\n",
    "  - 대형 언어모델의 원본 가중치는 고정하고, 각 선형층에 저랭크 보정만 학습하는 미세조정 기법\n",
    "  - 전체 파인튜닝 대비 학습 파라미터 수, VRAM, 시간을 절약할 수 있고\n",
    "  - 어댑터만 저장/배포하면 되어 경량 전송/버전관리에 이점이 있음\n",
    "  - 추론시 W` = W + AB로 계산하여 원본은 그대로, LoRA만 추가하는 방식\n",
    "  - 작업, 도메인에 맞춘 소규모 데이터로 빠르게 적용 가능\n",
    "  - 비용을 절감하고 속도를 높일 수 있으며, 여러 작업용 어댑터를 모듈처럼 교체 가능\n",
    "  - 베이스 모델이 이미 잘 하는 작업이라면 개선폭이 낮고, 데이터가 편향/협소하면 과적합 위험\n",
    "  - 항상 baseline과 지표 비교로 효과를 검증해야힘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e75fc1",
   "metadata": {},
   "source": [
    "lora에 맞춰 train, valid, test로 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1c6513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/lora\n",
    "!head -n 9500  data/nsmc_train_10k.jsonl > data/lora/train.jsonl\n",
    "!tail -n 500   data/nsmc_train_10k.jsonl > data/lora/valid.jsonl\n",
    "!cp data/nsmc_test_2k.jsonl              data/lora/test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f60403",
   "metadata": {},
   "source": [
    "mlx_lm lora의 주요 옵션 사용방법 정리\n",
    "\n",
    "- 핵심 옵션\n",
    "  - `--model <PATH|HF_ID>`\n",
    "    미세조정의 **베이스 모델**을 지정.\n",
    "    - 예: `models/qwen-0.5b`(로컬 변환본) 또는 `Qwen/Qwen2.5-0.5B-Instruct`(HF ID).\n",
    "    - 로컬 변환본이면 네트워크 없이 빠르고 안정적.\n",
    "  - `--adapter-path <DIR>`\n",
    "    **LoRA 어댑터 가중치 출력 위치**.\n",
    "    - 학습 중/종료 시 여기에 체크포인트가 저장됨.\n",
    "    - 여러 실험을 병행할 땐 경로를 실험명으로 구분(예: `adapters/nsmc-qwen0p5b-r8`).\n",
    "  - `--train`\n",
    "    **학습 모드**를 켜는 스위치. 없으면 학습이 시작되지 않음.\n",
    "  - `--data <DIR|FILE>`\n",
    "    **학습/검증/테스트 데이터 위치**.\n",
    "    - 디렉터리를 주면 내부의 `train.jsonl`, `valid.jsonl`, (선택) `test.jsonl`을 자동 인식.\n",
    "  - `--batch-size <INT>`\n",
    "    **최대 토큰 길이 × 배치 크기**가 메모리에 크게 영향.\n",
    "    - OOM 나면 `4 → 2`로 줄이거나, `--max-seq-length`로 시퀀스 길이를 낮추는 방식도 가능.\n",
    "  - `--iters <INT>`\n",
    "    **총 학습 스텝 수**. `epochs` 대신 쓰는 개념.\n",
    "    - 근사 환산: `steps ≈ (train_samples / batch_size) × epochs`\n",
    "    - 예) 10k 샘플, batch 8, 2 epochs → 약 2,500 steps.\n",
    "  - `--learning-rate <FLOAT>`\n",
    "    학습률. LoRA는 보통 `1e-4 ~ 3e-4` 사이에서 시작해 탐색.\n",
    "  - `--steps-per-report <INT>`\n",
    "    **로그/손실 보고 주기**(몇 스텝마다 콘솔에 학습 상태 출력할지). 너무 작으면 콘솔 스팸, 너무 크면 진행 파악이 어렵다 보통 20~50 권장.\n",
    "  - `--steps-per-eval <INT>`\n",
    "    **검증(Validation) 실행 주기**. 지정 스텝마다 `valid.jsonl` 일부로 평가.\n",
    "    - 빠른 루프 확인용으로 100~500 사이가 실용적.\n",
    "  - `--val-batches <INT>`\n",
    "    검증 시 **몇 배치**를 사용할지.\n",
    "    - 전체 검증을 매번 다 돌리면 느리니, 20~100 같은 범위로 “샘플링 평가”를 자주 하고, 마지막에 전체 평가를 별도로 돌리는 전략이 좋음.\n",
    "  - `--seed <INT>`\n",
    "    시드 고정. **재현성**을 위해 반드시 넣어두는 게 좋음(데이터 셔플·샘플링에 영향).\n",
    "\n",
    "- 자주 쓰는 보조 옵션\n",
    "  - `--fine-tune-type {lora,dora,full}`\n",
    "    **튜닝 방식** 선택. 기본은 LoRA. DORA/Full fine-tune은 자원 요구량↑.\n",
    "  - `--optimizer {adam,adamw,sgd,adafactor}`\n",
    "    옵티마이저 선택. LoRA에선 `adamw`가 무난.\n",
    "    - **SGD (Stochastic Gradient Descent)**: 가장 기본적인 최적화. 단순히 기울기만 보고 이동. 느리지만 단순/안정.\n",
    "    - **Adam**: 각 파라미터마다 학습률을 적응적으로 조절(모멘텀 + RMSProp). 대부분 기본값.\n",
    "    - **AdamW**: Adam + Weight Decay(가중치 감소 정규화). 과적합 방지에 더 효과적.\n",
    "    - **Adafactor**: 메모리 효율 극대화(대형 LM 튜닝에서 사용). Adam보다 가볍지만 불안정할 때도 있음.\n",
    "  - `--mask-prompt`\n",
    "    프롬프트 토큰 쪽 손실을 **마스킹**해 정답(assistant 부분)에 집중해 학습. 대화형 데이터에 유리.\n",
    "    - 대화 데이터는 `system` + `user` + `assistant`가 섞여 있음.\n",
    "    - 우리가 원하는 건 모델이 **“assistant 부분”만 잘 예측**하는 것.\n",
    "    - `mask-prompt`는 system/user 토큰의 loss를 0으로 만들어, 오로지 **assistant 응답 부분에 대해서만 학습**하도록 합니다.\n",
    "    - 안 쓰면 모델이 불필요하게 “user 메시지까지 흉내”내려다 성능이 떨어질 수 있어요.\n",
    "  - `--num-layers <INT>`\n",
    "    LoRA를 **상위 N개 레이어**에만 적용(자원 절약용). 모델 종속이라 과도하면 성능 하락 가능.\n",
    "    - LoRA는 모든 레이어에 보정행렬을 붙일 수 있지만,\n",
    "    - “상위 몇 개 레이어”만 적용해도 성능이 크게 올라가는 경우가 많습니다.\n",
    "    - 따라서 **num-layers=N**으로 제한하면 메모리/속도를 아끼면서 효과적인 튜닝이 가능합니다. (예: 24레이어 중 마지막 8개에만 LoRA 적용)\n",
    "  - `--max-seq-length <INT>`\n",
    "    최대 시퀀스 길이. 길수록 메모리↑. OOM 시 2048 → 1024로 낮추는 식으로 조정.\n",
    "  - `--save-every <INT>` / `--resume-adapter-file <PATH>`\n",
    "    **주기적 저장/재개**. 긴 러닝에서 유용. 중단 후 이어달리기에 사용.\n",
    "  - `--test` / `--test-batches <INT>`\n",
    "    학습 대신 **테스트 모드**로 `test.jsonl`을 빠르게 스캔해 품질을 훑어봄(정밀 지표는 별도 스크립트 추천).\n",
    "  - `--grad-checkpoint`\n",
    "    **그라드 체크포인팅**으로 메모리 사용을 줄이는 대신 연산량↑(속도 약간 느려짐).\n",
    "    - **grad-checkpoint**\n",
    "      - 역전파(backprop)에서 중간 계산 결과를 다 저장하지 않고, 필요한 시점에 **다시 계산**해서 메모리 절약하는 기법.\n",
    "      - 메모리↓, 속도는 조금 느려짐.\n",
    "      - GPU/Apple Silicon 메모리가 빡빡할 때 켜면 유용.\n",
    "    - **save-every**\n",
    "      - 학습 중 **몇 step마다 LoRA 어댑터를 디스크에 저장할지**.\n",
    "      - 장시간 학습하다 중단되더라도 여기서부터 재개 가능.\n",
    "      - 학습 알고리즘과 관계없고, 단순히 **체크포인트 파일 주기**에 해당.\n",
    "  - `--wandb <PROJECT_NAME>`\n",
    "    Weights & Biases 로깅 연동.\n",
    "\n",
    "- 운용 팁\n",
    "  - **데이터 디렉터리 구조 필수**: `--data data/lora/` 안에 `train.jsonl`, `valid.jsonl`(필수), `test.jsonl`(선택).\n",
    "  - **iters ↔ 시간 감**: 10k/bs8이면 1 epoch~1,250 steps 정도. M1 기준 1,500~2,500 steps가 “1회 완주 체감”에 적당.\n",
    "  - **검증 빈도**: `--steps-per-eval 200`, `--val-batches 50` 정도면 10k 소셋에서 과하지 않음.\n",
    "  - **로그 가독성**: `--steps-per-report 20~50` 권장.\n",
    "  - **베이스라인 비교**: 최종 평가는 `eval.py`(baseline vs adapter)로 **ACC/F1**을 같은 조건에서 두 번 찍어 **개선폭**을 기록."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c4813",
   "metadata": {},
   "source": [
    "> - **경사하강법이란?**\n",
    ">   - 머신러닝 모델은 보통 “손실 함수(loss)” 라는 성능 측정 식이 있습니다.\n",
    ">   - 예: 예측 값과 정답 사이의 차이(오차).\n",
    ">   - 우리가 원하는 건 손실이 최소가 되는 지점(최적 파라미터).\n",
    ">   - 그런데 수학적으로 한 번에 답을 구할 수 없을 때가 대부분 → 점진적으로 오차를 줄여 나가는 방법이 필요합니다.\n",
    ">   - 바로 그게 경사하강법이에요.\n",
    ">\n",
    "> - “경사(gradient)” = 현재 위치에서 손실이 얼마나, 어느 방향으로 증가하는지를 보여주는 기울기(미분 값).\n",
    "> - “하강(descent)” = 손실이 커지는 방향 반대로 조금씩 이동.\n",
    "> - 즉, 가중치를 기울기 반대 방향으로 조금씩 바꿔나가면서 손실을 줄여나가는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b925cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.073% (0.360M/494.033M)\n",
      "Starting training..., iters: 2500\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:38<00:00,  1.29it/s]\n",
      "Iter 1: Val loss 4.573, Val took 38.844s\n",
      "Iter 20: Train loss 2.032, Learning Rate 2.000e-04, It/sec 0.629, Tokens/sec 427.885, Trained Tokens 13606, Peak mem 4.479 GB\n",
      "Iter 40: Train loss 1.395, Learning Rate 2.000e-04, It/sec 0.723, Tokens/sec 468.038, Trained Tokens 26560, Peak mem 4.480 GB\n",
      "Iter 60: Train loss 1.403, Learning Rate 2.000e-04, It/sec 0.675, Tokens/sec 453.783, Trained Tokens 40013, Peak mem 4.480 GB\n",
      "Iter 80: Train loss 1.320, Learning Rate 2.000e-04, It/sec 0.751, Tokens/sec 488.534, Trained Tokens 53028, Peak mem 4.480 GB\n",
      "Iter 100: Train loss 1.396, Learning Rate 2.000e-04, It/sec 0.688, Tokens/sec 462.891, Trained Tokens 66489, Peak mem 4.480 GB\n",
      "Iter 100: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000100_adapters.safetensors.\n",
      "Iter 120: Train loss 1.341, Learning Rate 2.000e-04, It/sec 0.714, Tokens/sec 469.355, Trained Tokens 79639, Peak mem 4.482 GB\n",
      "Iter 140: Train loss 1.312, Learning Rate 2.000e-04, It/sec 0.753, Tokens/sec 489.810, Trained Tokens 92653, Peak mem 4.482 GB\n",
      "Iter 160: Train loss 1.310, Learning Rate 2.000e-04, It/sec 0.741, Tokens/sec 483.291, Trained Tokens 105690, Peak mem 4.482 GB\n",
      "Iter 180: Train loss 1.257, Learning Rate 2.000e-04, It/sec 0.734, Tokens/sec 470.795, Trained Tokens 118517, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:39<00:00,  1.25it/s]\n",
      "Iter 200: Val loss 1.386, Val took 39.898s\n",
      "Iter 200: Train loss 1.354, Learning Rate 2.000e-04, It/sec 0.717, Tokens/sec 469.578, Trained Tokens 131618, Peak mem 4.482 GB\n",
      "Iter 200: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000200_adapters.safetensors.\n",
      "Iter 220: Train loss 1.307, Learning Rate 2.000e-04, It/sec 0.688, Tokens/sec 455.571, Trained Tokens 144868, Peak mem 4.482 GB\n",
      "Iter 240: Train loss 1.394, Learning Rate 2.000e-04, It/sec 0.680, Tokens/sec 452.609, Trained Tokens 158183, Peak mem 4.482 GB\n",
      "Iter 260: Train loss 1.387, Learning Rate 2.000e-04, It/sec 0.645, Tokens/sec 434.629, Trained Tokens 171670, Peak mem 4.482 GB\n",
      "Iter 280: Train loss 1.276, Learning Rate 2.000e-04, It/sec 0.748, Tokens/sec 486.842, Trained Tokens 184685, Peak mem 4.482 GB\n",
      "Iter 300: Train loss 1.250, Learning Rate 2.000e-04, It/sec 0.659, Tokens/sec 428.372, Trained Tokens 197677, Peak mem 4.482 GB\n",
      "Iter 300: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000300_adapters.safetensors.\n",
      "Iter 320: Train loss 1.415, Learning Rate 2.000e-04, It/sec 0.617, Tokens/sec 422.406, Trained Tokens 211376, Peak mem 4.482 GB\n",
      "Iter 340: Train loss 1.433, Learning Rate 2.000e-04, It/sec 0.586, Tokens/sec 408.473, Trained Tokens 225315, Peak mem 4.482 GB\n",
      "Iter 360: Train loss 1.299, Learning Rate 2.000e-04, It/sec 0.635, Tokens/sec 412.823, Trained Tokens 238319, Peak mem 4.482 GB\n",
      "Iter 380: Train loss 1.331, Learning Rate 2.000e-04, It/sec 0.656, Tokens/sec 437.725, Trained Tokens 251661, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:44<00:00,  1.13it/s]\n",
      "Iter 400: Val loss 1.323, Val took 44.154s\n",
      "Iter 400: Train loss 1.331, Learning Rate 2.000e-04, It/sec 0.667, Tokens/sec 444.268, Trained Tokens 264985, Peak mem 4.482 GB\n",
      "Iter 400: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000400_adapters.safetensors.\n",
      "Iter 420: Train loss 1.450, Learning Rate 2.000e-04, It/sec 0.539, Tokens/sec 376.765, Trained Tokens 278974, Peak mem 4.482 GB\n",
      "Iter 440: Train loss 1.195, Learning Rate 2.000e-04, It/sec 0.685, Tokens/sec 441.447, Trained Tokens 291861, Peak mem 4.482 GB\n",
      "Iter 460: Train loss 1.326, Learning Rate 2.000e-04, It/sec 0.574, Tokens/sec 384.172, Trained Tokens 305249, Peak mem 4.482 GB\n",
      "Iter 480: Train loss 1.410, Learning Rate 2.000e-04, It/sec 0.596, Tokens/sec 408.394, Trained Tokens 318944, Peak mem 4.482 GB\n",
      "Iter 500: Train loss 1.382, Learning Rate 2.000e-04, It/sec 0.593, Tokens/sec 398.736, Trained Tokens 332390, Peak mem 4.482 GB\n",
      "Iter 500: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000500_adapters.safetensors.\n",
      "Iter 520: Train loss 1.445, Learning Rate 2.000e-04, It/sec 0.438, Tokens/sec 304.023, Trained Tokens 346272, Peak mem 4.482 GB\n",
      "Iter 540: Train loss 1.279, Learning Rate 2.000e-04, It/sec 0.718, Tokens/sec 467.399, Trained Tokens 359290, Peak mem 4.482 GB\n",
      "Iter 560: Train loss 1.296, Learning Rate 2.000e-04, It/sec 0.724, Tokens/sec 481.454, Trained Tokens 372583, Peak mem 4.482 GB\n",
      "Iter 580: Train loss 1.289, Learning Rate 2.000e-04, It/sec 0.683, Tokens/sec 450.683, Trained Tokens 385787, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:40<00:00,  1.24it/s]\n",
      "Iter 600: Val loss 1.336, Val took 40.269s\n",
      "Iter 600: Train loss 1.245, Learning Rate 2.000e-04, It/sec 0.666, Tokens/sec 432.600, Trained Tokens 398781, Peak mem 4.482 GB\n",
      "Iter 600: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000600_adapters.safetensors.\n",
      "Iter 620: Train loss 1.285, Learning Rate 2.000e-04, It/sec 0.704, Tokens/sec 462.683, Trained Tokens 411921, Peak mem 4.482 GB\n",
      "Iter 640: Train loss 1.247, Learning Rate 2.000e-04, It/sec 0.777, Tokens/sec 498.772, Trained Tokens 424759, Peak mem 4.482 GB\n",
      "Iter 660: Train loss 1.407, Learning Rate 2.000e-04, It/sec 0.670, Tokens/sec 455.673, Trained Tokens 438360, Peak mem 4.482 GB\n",
      "Iter 680: Train loss 1.300, Learning Rate 2.000e-04, It/sec 0.758, Tokens/sec 502.471, Trained Tokens 451613, Peak mem 4.482 GB\n",
      "Iter 700: Train loss 1.299, Learning Rate 2.000e-04, It/sec 0.701, Tokens/sec 474.097, Trained Tokens 465144, Peak mem 4.482 GB\n",
      "Iter 700: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000700_adapters.safetensors.\n",
      "Iter 720: Train loss 1.252, Learning Rate 2.000e-04, It/sec 0.730, Tokens/sec 476.271, Trained Tokens 478184, Peak mem 4.482 GB\n",
      "Iter 740: Train loss 1.332, Learning Rate 2.000e-04, It/sec 0.675, Tokens/sec 455.463, Trained Tokens 491683, Peak mem 4.482 GB\n",
      "Iter 760: Train loss 1.285, Learning Rate 2.000e-04, It/sec 0.702, Tokens/sec 457.706, Trained Tokens 504721, Peak mem 4.482 GB\n",
      "Iter 780: Train loss 1.337, Learning Rate 2.000e-04, It/sec 0.654, Tokens/sec 440.605, Trained Tokens 518204, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:38<00:00,  1.29it/s]\n",
      "Iter 800: Val loss 1.342, Val took 38.849s\n",
      "Iter 800: Train loss 1.340, Learning Rate 2.000e-04, It/sec 0.670, Tokens/sec 453.092, Trained Tokens 531722, Peak mem 4.482 GB\n",
      "Iter 800: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000800_adapters.safetensors.\n",
      "Iter 820: Train loss 1.349, Learning Rate 2.000e-04, It/sec 0.696, Tokens/sec 466.128, Trained Tokens 545112, Peak mem 4.482 GB\n",
      "Iter 840: Train loss 1.362, Learning Rate 2.000e-04, It/sec 0.641, Tokens/sec 437.055, Trained Tokens 558750, Peak mem 4.482 GB\n",
      "Iter 860: Train loss 1.309, Learning Rate 2.000e-04, It/sec 0.710, Tokens/sec 471.688, Trained Tokens 572043, Peak mem 4.482 GB\n",
      "Iter 880: Train loss 1.265, Learning Rate 2.000e-04, It/sec 0.717, Tokens/sec 468.592, Trained Tokens 585118, Peak mem 4.482 GB\n",
      "Iter 900: Train loss 1.205, Learning Rate 2.000e-04, It/sec 0.774, Tokens/sec 493.600, Trained Tokens 597867, Peak mem 4.482 GB\n",
      "Iter 900: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0000900_adapters.safetensors.\n",
      "Iter 920: Train loss 1.350, Learning Rate 2.000e-04, It/sec 0.718, Tokens/sec 482.605, Trained Tokens 611314, Peak mem 4.482 GB\n",
      "Iter 940: Train loss 1.259, Learning Rate 2.000e-04, It/sec 0.713, Tokens/sec 473.927, Trained Tokens 624603, Peak mem 4.482 GB\n",
      "Iter 960: Train loss 1.356, Learning Rate 2.000e-04, It/sec 0.717, Tokens/sec 483.835, Trained Tokens 638108, Peak mem 4.482 GB\n",
      "Iter 980: Train loss 1.388, Learning Rate 2.000e-04, It/sec 0.681, Tokens/sec 471.621, Trained Tokens 651965, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:40<00:00,  1.24it/s]\n",
      "Iter 1000: Val loss 1.337, Val took 40.322s\n",
      "Iter 1000: Train loss 1.301, Learning Rate 2.000e-04, It/sec 0.681, Tokens/sec 454.358, Trained Tokens 665311, Peak mem 4.482 GB\n",
      "Iter 1000: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001000_adapters.safetensors.\n",
      "Iter 1020: Train loss 1.311, Learning Rate 2.000e-04, It/sec 0.736, Tokens/sec 483.310, Trained Tokens 678441, Peak mem 4.482 GB\n",
      "Iter 1040: Train loss 1.254, Learning Rate 2.000e-04, It/sec 0.741, Tokens/sec 485.006, Trained Tokens 691526, Peak mem 4.482 GB\n",
      "Iter 1060: Train loss 1.300, Learning Rate 2.000e-04, It/sec 0.734, Tokens/sec 490.913, Trained Tokens 704904, Peak mem 4.482 GB\n",
      "Iter 1080: Train loss 1.270, Learning Rate 2.000e-04, It/sec 0.653, Tokens/sec 430.941, Trained Tokens 718099, Peak mem 4.482 GB\n",
      "Iter 1100: Train loss 1.316, Learning Rate 2.000e-04, It/sec 0.719, Tokens/sec 480.586, Trained Tokens 731471, Peak mem 4.482 GB\n",
      "Iter 1100: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001100_adapters.safetensors.\n",
      "Iter 1120: Train loss 1.352, Learning Rate 2.000e-04, It/sec 0.693, Tokens/sec 470.801, Trained Tokens 745065, Peak mem 4.482 GB\n",
      "Iter 1140: Train loss 1.341, Learning Rate 2.000e-04, It/sec 0.641, Tokens/sec 435.721, Trained Tokens 758654, Peak mem 4.482 GB\n",
      "Iter 1160: Train loss 1.221, Learning Rate 2.000e-04, It/sec 0.740, Tokens/sec 480.546, Trained Tokens 771634, Peak mem 4.482 GB\n",
      "Iter 1180: Train loss 1.234, Learning Rate 2.000e-04, It/sec 0.756, Tokens/sec 495.722, Trained Tokens 784757, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:39<00:00,  1.25it/s]\n",
      "Iter 1200: Val loss 1.331, Val took 39.978s\n",
      "Iter 1200: Train loss 1.332, Learning Rate 2.000e-04, It/sec 0.648, Tokens/sec 438.861, Trained Tokens 798308, Peak mem 4.482 GB\n",
      "Iter 1200: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001200_adapters.safetensors.\n",
      "Iter 1220: Train loss 1.298, Learning Rate 2.000e-04, It/sec 0.657, Tokens/sec 447.852, Trained Tokens 811947, Peak mem 4.482 GB\n",
      "Iter 1240: Train loss 1.309, Learning Rate 2.000e-04, It/sec 0.694, Tokens/sec 468.322, Trained Tokens 825439, Peak mem 4.482 GB\n",
      "Iter 1260: Train loss 1.459, Learning Rate 2.000e-04, It/sec 0.625, Tokens/sec 442.902, Trained Tokens 839616, Peak mem 4.482 GB\n",
      "Iter 1280: Train loss 1.268, Learning Rate 2.000e-04, It/sec 0.683, Tokens/sec 455.267, Trained Tokens 852956, Peak mem 4.482 GB\n",
      "Iter 1300: Train loss 1.351, Learning Rate 2.000e-04, It/sec 0.700, Tokens/sec 474.679, Trained Tokens 866525, Peak mem 4.482 GB\n",
      "Iter 1300: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001300_adapters.safetensors.\n",
      "Iter 1320: Train loss 1.214, Learning Rate 2.000e-04, It/sec 0.743, Tokens/sec 492.579, Trained Tokens 879776, Peak mem 4.482 GB\n",
      "Iter 1340: Train loss 1.309, Learning Rate 2.000e-04, It/sec 0.654, Tokens/sec 445.636, Trained Tokens 893394, Peak mem 4.482 GB\n",
      "Iter 1360: Train loss 1.308, Learning Rate 2.000e-04, It/sec 0.721, Tokens/sec 484.027, Trained Tokens 906824, Peak mem 4.482 GB\n",
      "Iter 1380: Train loss 1.283, Learning Rate 2.000e-04, It/sec 0.725, Tokens/sec 480.780, Trained Tokens 920096, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:40<00:00,  1.24it/s]\n",
      "Iter 1400: Val loss 1.369, Val took 40.392s\n",
      "Iter 1400: Train loss 1.341, Learning Rate 2.000e-04, It/sec 0.707, Tokens/sec 483.052, Trained Tokens 933769, Peak mem 4.482 GB\n",
      "Iter 1400: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001400_adapters.safetensors.\n",
      "Iter 1420: Train loss 1.229, Learning Rate 2.000e-04, It/sec 0.682, Tokens/sec 452.516, Trained Tokens 947034, Peak mem 4.482 GB\n",
      "Iter 1440: Train loss 1.215, Learning Rate 2.000e-04, It/sec 0.716, Tokens/sec 467.963, Trained Tokens 960108, Peak mem 4.482 GB\n",
      "Iter 1460: Train loss 1.254, Learning Rate 2.000e-04, It/sec 0.695, Tokens/sec 463.318, Trained Tokens 973438, Peak mem 4.482 GB\n",
      "Iter 1480: Train loss 1.261, Learning Rate 2.000e-04, It/sec 0.740, Tokens/sec 487.092, Trained Tokens 986611, Peak mem 4.482 GB\n",
      "Iter 1500: Train loss 1.318, Learning Rate 2.000e-04, It/sec 0.685, Tokens/sec 460.878, Trained Tokens 1000077, Peak mem 4.482 GB\n",
      "Iter 1500: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001500_adapters.safetensors.\n",
      "Iter 1520: Train loss 1.169, Learning Rate 2.000e-04, It/sec 0.762, Tokens/sec 487.577, Trained Tokens 1012867, Peak mem 4.482 GB\n",
      "Iter 1540: Train loss 1.265, Learning Rate 2.000e-04, It/sec 0.757, Tokens/sec 499.254, Trained Tokens 1026058, Peak mem 4.482 GB\n",
      "Iter 1560: Train loss 1.218, Learning Rate 2.000e-04, It/sec 0.701, Tokens/sec 459.299, Trained Tokens 1039168, Peak mem 4.482 GB\n",
      "Iter 1580: Train loss 1.282, Learning Rate 2.000e-04, It/sec 0.641, Tokens/sec 428.090, Trained Tokens 1052515, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:37<00:00,  1.32it/s]\n",
      "Iter 1600: Val loss 1.313, Val took 37.763s\n",
      "Iter 1600: Train loss 1.283, Learning Rate 2.000e-04, It/sec 0.666, Tokens/sec 452.425, Trained Tokens 1066095, Peak mem 4.482 GB\n",
      "Iter 1600: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001600_adapters.safetensors.\n",
      "Iter 1620: Train loss 1.185, Learning Rate 2.000e-04, It/sec 0.779, Tokens/sec 504.997, Trained Tokens 1079062, Peak mem 4.482 GB\n",
      "Iter 1640: Train loss 1.194, Learning Rate 2.000e-04, It/sec 0.750, Tokens/sec 485.905, Trained Tokens 1092023, Peak mem 4.482 GB\n",
      "Iter 1660: Train loss 1.236, Learning Rate 2.000e-04, It/sec 0.707, Tokens/sec 468.056, Trained Tokens 1105269, Peak mem 4.482 GB\n",
      "Iter 1680: Train loss 1.198, Learning Rate 2.000e-04, It/sec 0.723, Tokens/sec 472.546, Trained Tokens 1118341, Peak mem 4.482 GB\n",
      "Iter 1700: Train loss 1.253, Learning Rate 2.000e-04, It/sec 0.700, Tokens/sec 463.756, Trained Tokens 1131590, Peak mem 4.482 GB\n",
      "Iter 1700: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001700_adapters.safetensors.\n",
      "Iter 1720: Train loss 1.255, Learning Rate 2.000e-04, It/sec 0.683, Tokens/sec 460.049, Trained Tokens 1145056, Peak mem 4.482 GB\n",
      "Iter 1740: Train loss 1.312, Learning Rate 2.000e-04, It/sec 0.659, Tokens/sec 446.243, Trained Tokens 1158589, Peak mem 4.482 GB\n",
      "Iter 1760: Train loss 1.314, Learning Rate 2.000e-04, It/sec 0.638, Tokens/sec 430.398, Trained Tokens 1172084, Peak mem 4.482 GB\n",
      "Iter 1780: Train loss 1.260, Learning Rate 2.000e-04, It/sec 0.708, Tokens/sec 468.154, Trained Tokens 1185317, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:38<00:00,  1.31it/s]\n",
      "Iter 1800: Val loss 1.338, Val took 38.140s\n",
      "Iter 1800: Train loss 1.207, Learning Rate 2.000e-04, It/sec 0.756, Tokens/sec 490.711, Trained Tokens 1198294, Peak mem 4.482 GB\n",
      "Iter 1800: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001800_adapters.safetensors.\n",
      "Iter 1820: Train loss 1.186, Learning Rate 2.000e-04, It/sec 0.759, Tokens/sec 487.231, Trained Tokens 1211141, Peak mem 4.482 GB\n",
      "Iter 1840: Train loss 1.284, Learning Rate 2.000e-04, It/sec 0.790, Tokens/sec 521.497, Trained Tokens 1224344, Peak mem 4.482 GB\n",
      "Iter 1860: Train loss 1.325, Learning Rate 2.000e-04, It/sec 0.722, Tokens/sec 488.013, Trained Tokens 1237866, Peak mem 4.482 GB\n",
      "Iter 1880: Train loss 1.267, Learning Rate 2.000e-04, It/sec 0.714, Tokens/sec 474.260, Trained Tokens 1251150, Peak mem 4.482 GB\n",
      "Iter 1900: Train loss 1.257, Learning Rate 2.000e-04, It/sec 0.674, Tokens/sec 449.522, Trained Tokens 1264494, Peak mem 4.482 GB\n",
      "Iter 1900: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0001900_adapters.safetensors.\n",
      "Iter 1920: Train loss 1.264, Learning Rate 2.000e-04, It/sec 0.674, Tokens/sec 453.532, Trained Tokens 1277945, Peak mem 4.482 GB\n",
      "Iter 1940: Train loss 1.227, Learning Rate 2.000e-04, It/sec 0.777, Tokens/sec 509.343, Trained Tokens 1291054, Peak mem 4.482 GB\n",
      "Iter 1960: Train loss 1.316, Learning Rate 2.000e-04, It/sec 0.705, Tokens/sec 469.000, Trained Tokens 1304356, Peak mem 4.482 GB\n",
      "Iter 1980: Train loss 1.225, Learning Rate 2.000e-04, It/sec 0.748, Tokens/sec 485.923, Trained Tokens 1317349, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:38<00:00,  1.30it/s]\n",
      "Iter 2000: Val loss 1.334, Val took 38.500s\n",
      "Iter 2000: Train loss 1.236, Learning Rate 2.000e-04, It/sec 0.729, Tokens/sec 486.121, Trained Tokens 1330693, Peak mem 4.482 GB\n",
      "Iter 2000: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0002000_adapters.safetensors.\n",
      "Iter 2020: Train loss 1.253, Learning Rate 2.000e-04, It/sec 0.689, Tokens/sec 455.294, Trained Tokens 1343901, Peak mem 4.482 GB\n",
      "Iter 2040: Train loss 1.275, Learning Rate 2.000e-04, It/sec 0.693, Tokens/sec 467.617, Trained Tokens 1357403, Peak mem 4.482 GB\n",
      "Iter 2060: Train loss 1.239, Learning Rate 2.000e-04, It/sec 0.699, Tokens/sec 457.074, Trained Tokens 1370484, Peak mem 4.482 GB\n",
      "Iter 2080: Train loss 1.184, Learning Rate 2.000e-04, It/sec 0.792, Tokens/sec 513.871, Trained Tokens 1383459, Peak mem 4.482 GB\n",
      "Iter 2100: Train loss 1.175, Learning Rate 2.000e-04, It/sec 0.791, Tokens/sec 514.711, Trained Tokens 1396474, Peak mem 4.482 GB\n",
      "Iter 2100: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0002100_adapters.safetensors.\n",
      "Iter 2120: Train loss 1.213, Learning Rate 2.000e-04, It/sec 0.734, Tokens/sec 485.071, Trained Tokens 1409688, Peak mem 4.482 GB\n",
      "Iter 2140: Train loss 1.358, Learning Rate 2.000e-04, It/sec 0.708, Tokens/sec 491.543, Trained Tokens 1423564, Peak mem 4.482 GB\n",
      "Iter 2160: Train loss 1.217, Learning Rate 2.000e-04, It/sec 0.773, Tokens/sec 501.504, Trained Tokens 1436536, Peak mem 4.482 GB\n",
      "Iter 2180: Train loss 1.171, Learning Rate 2.000e-04, It/sec 0.782, Tokens/sec 502.832, Trained Tokens 1449397, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:37<00:00,  1.33it/s]\n",
      "Iter 2200: Val loss 1.301, Val took 37.527s\n",
      "Iter 2200: Train loss 1.270, Learning Rate 2.000e-04, It/sec 0.725, Tokens/sec 486.087, Trained Tokens 1462803, Peak mem 4.482 GB\n",
      "Iter 2200: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0002200_adapters.safetensors.\n",
      "Iter 2220: Train loss 1.265, Learning Rate 2.000e-04, It/sec 0.717, Tokens/sec 473.336, Trained Tokens 1476014, Peak mem 4.482 GB\n",
      "Iter 2240: Train loss 1.311, Learning Rate 2.000e-04, It/sec 0.690, Tokens/sec 467.248, Trained Tokens 1489553, Peak mem 4.482 GB\n",
      "Iter 2260: Train loss 1.174, Learning Rate 2.000e-04, It/sec 0.725, Tokens/sec 473.798, Trained Tokens 1502626, Peak mem 4.482 GB\n",
      "Iter 2280: Train loss 1.210, Learning Rate 2.000e-04, It/sec 0.733, Tokens/sec 485.277, Trained Tokens 1515872, Peak mem 4.482 GB\n",
      "Iter 2300: Train loss 1.306, Learning Rate 2.000e-04, It/sec 0.669, Tokens/sec 449.009, Trained Tokens 1529298, Peak mem 4.482 GB\n",
      "Iter 2300: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0002300_adapters.safetensors.\n",
      "Iter 2320: Train loss 1.395, Learning Rate 2.000e-04, It/sec 0.699, Tokens/sec 484.872, Trained Tokens 1543168, Peak mem 4.482 GB\n",
      "Iter 2340: Train loss 1.335, Learning Rate 2.000e-04, It/sec 0.700, Tokens/sec 470.417, Trained Tokens 1556609, Peak mem 4.482 GB\n",
      "Iter 2360: Train loss 1.253, Learning Rate 2.000e-04, It/sec 0.728, Tokens/sec 480.561, Trained Tokens 1569820, Peak mem 4.482 GB\n",
      "Iter 2380: Train loss 1.297, Learning Rate 2.000e-04, It/sec 0.743, Tokens/sec 497.740, Trained Tokens 1583219, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [00:37<00:00,  1.32it/s]\n",
      "Iter 2400: Val loss 1.318, Val took 37.794s\n",
      "Iter 2400: Train loss 1.360, Learning Rate 2.000e-04, It/sec 0.676, Tokens/sec 469.613, Trained Tokens 1597112, Peak mem 4.482 GB\n",
      "Iter 2400: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0002400_adapters.safetensors.\n",
      "Iter 2420: Train loss 1.197, Learning Rate 2.000e-04, It/sec 0.741, Tokens/sec 484.257, Trained Tokens 1610186, Peak mem 4.482 GB\n",
      "Iter 2440: Train loss 1.164, Learning Rate 2.000e-04, It/sec 0.724, Tokens/sec 469.210, Trained Tokens 1623142, Peak mem 4.482 GB\n",
      "Iter 2460: Train loss 1.332, Learning Rate 2.000e-04, It/sec 0.651, Tokens/sec 453.196, Trained Tokens 1637058, Peak mem 4.482 GB\n",
      "Iter 2480: Train loss 1.132, Learning Rate 2.000e-04, It/sec 0.763, Tokens/sec 491.440, Trained Tokens 1649939, Peak mem 4.482 GB\n",
      "Calculating loss...: 100%|██████████████████████| 50/50 [01:52<00:00,  2.24s/it]\n",
      "Iter 2500: Val loss 1.329, Val took 39.815s\n",
      "Iter 2500: Train loss 1.189, Learning Rate 2.000e-04, It/sec 0.715, Tokens/sec 473.454, Trained Tokens 1663177, Peak mem 4.482 GB\n",
      "Iter 2500: Saved adapter weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors and adapters/nsmc-qwen0p5b-r8/0002500_adapters.safetensors.\n",
      "Saved final weights to adapters/nsmc-qwen0p5b-r8/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p adapters\n",
    "!uv run python -m mlx_lm lora \\\n",
    "  --model models/qwen-0.5b \\\n",
    "  --adapter-path adapters/nsmc-qwen0p5b-r8 \\\n",
    "  --train \\\n",
    "  --data data/lora \\\n",
    "  --batch-size 8 \\\n",
    "  --iters 2500 \\\n",
    "  --learning-rate 2e-4 \\\n",
    "  --steps-per-report 20 \\\n",
    "  --steps-per-eval 200 \\\n",
    "  --val-batches 50 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab4fc9",
   "metadata": {},
   "source": [
    "## 결과확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ce540",
   "metadata": {},
   "source": [
    "테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04052190",
   "metadata": {},
   "source": [
    "- `--adapter-path`: 모델에 합성할 LoRA 가중치\n",
    "- `--test`: 손실만 계산하는 루프 실행\n",
    "- `--test-batches`: 루프 최대값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff76b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Testing\n",
      "Calculating loss...: 100%|████████████████████| 250/250 [01:23<00:00,  2.98it/s]\n",
      "Test loss 1.278, Test ppl 3.590.\n"
     ]
    }
   ],
   "source": [
    "# Test the LoRA adapter\n",
    "!uv run python -m mlx_lm lora \\\n",
    "  --model models/qwen-0.5b \\\n",
    "  --adapter-path adapters/nsmc-qwen0p5b-r8 \\\n",
    "  --test \\\n",
    "  --data data/lora \\\n",
    "  --test-batches 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44667e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Testing\n",
      "Calculating loss...: 100%|████████████████████| 250/250 [01:21<00:00,  3.05it/s]\n",
      "Test loss 4.585, Test ppl 98.004.\n"
     ]
    }
   ],
   "source": [
    "# Test the baseline\n",
    "!uv run python -m mlx_lm lora \\\n",
    "    --model models/qwen-0.5b \\\n",
    "    --adapter-path \"\" \\\n",
    "    --test \\\n",
    "    --data data/lora \\\n",
    "    --test-batches 250\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980a2d3",
   "metadata": {},
   "source": [
    "- **테스트결과**\n",
    "  - LoRA 어댑터 적용: Test Loss 1.278 / Perplexity 3.50\n",
    "  - BaseLine : Test Loss 4.585 / Perplexity 98.0\n",
    "  - ppl이 낮아짐, 튜닝후 정답 토큰을 3.59개 후보 중 하나 정도로 봄\n",
    "  - 태스크 정확도(ACC/F1)는 따로 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0e336",
   "metadata": {},
   "source": [
    "평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88e9fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144203, 29281]\n",
      "[63089, 29281]\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load\n",
    "model, tok = load(\"models/qwen-0.5b\")\n",
    "\n",
    "print(tok.encode(\"긍정\"))\n",
    "print(tok.encode(\"부정\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad99707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"role\": \"system\", \"content\": \"너는 한국어 감성분석 어시스턴트야.\"}, {\"role\": \"user\", \"content\": \"다음 리뷰의 감성을 한 단어로만 답해줘(긍정/부정). 리뷰: \"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "text = \"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"너는 한국어 감성분석 어시스턴트야.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"다음 리뷰의 감성을 한 단어로만 답해줘(긍정/부정). 리뷰: {text}\"}\n",
    "]\n",
    "prompt = \", \".join([json.dumps(m, ensure_ascii=False) for m in messages])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b84e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bachtaeyeong/10_SrcHub/AI-Service-Lab/week01-model-finetune/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseLine ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2000 [00:00<06:11,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real output: 부정\n",
      "\n",
      "이, normalized: <re.Match object; span=(0, 2), match='부정'>\n",
      "real output: 부정\n",
      "\n",
      "이, normalized: <re.Match object; span=(0, 2), match='부정'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2000 [00:00<05:54,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real output: 부정\n",
      "\n",
      "이, normalized: <re.Match object; span=(0, 2), match='부정'>\n",
      "real output: 부정\n",
      "\n",
      "이, normalized: <re.Match object; span=(0, 2), match='부정'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:58<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real output: 부정\n",
      "\n",
      "부, normalized: <re.Match object; span=(0, 2), match='부정'>\n",
      "ACC=0.4912 (on 1999 classified samples)\n",
      "Unclassified=1 / 2000\n",
      "Baseline ACC=0.4910\n",
      "LoRA -------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 2/2000 [00:00<06:11,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real output: 부정. �, normalized: <re.Match object; span=(0, 2), match='부정'>\n",
      "real output: 부정. , normalized: <re.Match object; span=(0, 2), match='부정'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2000 [00:00<05:59,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real output: 부정 1, normalized: <re.Match object; span=(0, 2), match='부정'>\n",
      "real output: 부정... , normalized: <re.Match object; span=(0, 2), match='부정'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:17<00:00,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real output: 부정. , normalized: <re.Match object; span=(0, 2), match='부정'>\n",
      "ACC=0.6379 (on 1936 classified samples)\n",
      "Unclassified=64 / 2000\n",
      "LoRA ACC=0.6175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts/eval.py (수정본)\n",
    "import re, json, tqdm\n",
    "from datasets import load_dataset\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "MODEL_PATH = \"models/qwen-0.5b\"\n",
    "ADAPTER_PATH = \"adapters/nsmc-qwen0p5b-r8\"\n",
    "TEST_PATH = \"data/lora/test.jsonl\"\n",
    "IDX=0\n",
    "\n",
    "def extract_text_and_label(rec):\n",
    "    # chat 형식: system/user/assistant 중 user와 assistant만 사용\n",
    "    msgs = rec[\"messages\"]\n",
    "    user_text = next((m[\"content\"] for m in msgs if m[\"role\"] == \"user\"), \"\")\n",
    "    gold = next((m[\"content\"] for m in msgs if m[\"role\"] == \"assistant\"), \"\").strip()\n",
    "    # 혹시 공백/장식이 섞여도 '긍정|부정'만 남기기\n",
    "    gold = \"긍정\" if \"긍정\" in gold else \"부정\"\n",
    "    return user_text, gold\n",
    "\n",
    "def ask(text, model, tokenizer):\n",
    "    global IDX\n",
    "    IDX+=1\n",
    "    prompt = (\n",
    "        \"너는 한국어 감성분석 어시스턴트야.\\n\"\n",
    "        \"다음 리뷰의 감성을 딱 한 단어로만 출력해. 다른 말/기호 금지.\\n\"\n",
    "        \"가능한 답: 긍정 또는 부정\\n\"\n",
    "        f\"리뷰: {text}\\n\"\n",
    "        \"정답:\"\n",
    "    )\n",
    "    out = generate(model, tokenizer, prompt=prompt, max_tokens=4)\n",
    "    m = re.search(\"(긍정|부정)\", out)\n",
    "    if (IDX%2000)<5:\n",
    "        print(f\"real output: {out}, normalized: {m}\")\n",
    "    return m.group(1) if m else None   # ★ fallback을 None으로\n",
    "\n",
    "def evaluate(model, tokenizer, ds, limit=2000):\n",
    "    ds = ds.select(range(min(limit, len(ds))))\n",
    "    ok = 0\n",
    "    ok, total, unclassified = 0, 0, 0\n",
    "    for rec in tqdm.tqdm(ds):\n",
    "        text, gold = extract_text_and_label(rec)\n",
    "        pred = ask(text, model, tokenizer)\n",
    "        if pred is None:\n",
    "            unclassified += 1\n",
    "        else:\n",
    "            total += 1\n",
    "            ok += (pred == gold)\n",
    "    \n",
    "    acc = ok/total if total > 0 else 0\n",
    "    print(f\"ACC={acc:.4f} (on {total} classified samples)\")\n",
    "    print(f\"Unclassified={unclassified} / {len(ds)}\")\n",
    "    return ok / len(ds)\n",
    "\n",
    "def main():\n",
    "    ds_te = load_dataset(\"json\", data_files={\"test\": TEST_PATH})[\"test\"]\n",
    "    base_model, base_tok = load(MODEL_PATH, adapter_path=ADAPTER_PATH)\n",
    "    acc_base = evaluate(base_model, base_tok, ds_te, limit=2000)\n",
    "    print(f\"Baseline ACC={acc_base:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991825e",
   "metadata": {},
   "source": [
    "- **1차 평가결과**\n",
    "- NSMC 감성 분류 (Qwen-0.5B, M1 Pro, LoRA 튜닝)\n",
    "\n",
    "    | 모델          | Test Loss | Test PPL | ACC (2000 샘플) | 소요 시간 |\n",
    "    |---------------|-----------|----------|-----------------|-----------|\n",
    "    | Baseline      | 4.585     | 98.00    | 0.2775          | 5m 49s    |\n",
    "    | LoRA (r=8)    | 1.278     | 3.59     | 0.0010          | 6m 02s    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e940ca0",
   "metadata": {},
   "source": [
    "> - `LoRA의 r=8` : \n",
    ">   - 작으면 적은 메모리로 빠른학습, 크면 더 많은 표현력이며 느리고 메모리 더 필요. \n",
    ">   - r=8이면 dxd 행렬 대신 2xdx8짜리 행렬 학습.\n",
    "> - `Test Loss`:\n",
    ">   - 모델이 정답 분포와 얼마나 멀리 떨어져 있는지, 여기서는 cross-entropy loss로 계산\n",
    ">   - 값이 작을 수록 모델이 \"정답 확률에 더 높은 확률을 준다는 뜻\"\n",
    "> - `Perplexity(PPL)`: \n",
    ">  - 모델이 다음 토큰을 맞추는 데 평균적으로 몇 가지 선택지 사이에서 헷갈리는지\n",
    ">  - ppl=1이면 항상 정답만, ppl=3.5면 3-4개 후보중, ppl=98이면 거의 랜덤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea33142",
   "metadata": {},
   "source": [
    "- **2차 평가결과**\n",
    "  - 1차 평가중 생성된 데이터를 출력해보니 불필요한 텍스트를 자꾸 덧붙임.\n",
    "  - 또한 긍정적/부정적이다 등 분류자체는 성공했으나 형식상 실패도 보임\n",
    "  - 프롬프트를 수정하여 포맷을 일치화하도록 하고, 정규식 사용해 형식면에서도 수정\n",
    "    | 모델          | Test Loss | Test PPL | ACC (2000 샘플) | 소요 시간 |\n",
    "    |---------------|-----------|----------|-----------------|-----------|\n",
    "    | Baseline      | 4.585     | 98.00    | 0.4910          | 5m 58s    |\n",
    "    | LoRA (r=8)    | 1.278     | 3.59     | 0.6175          | 5m 30s    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be80e5",
   "metadata": {},
   "source": [
    "중간 체크포인트중 후보 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d55bb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Testing\n",
      "Calculating loss...: 100%|████████████████████| 250/250 [01:23<00:00,  2.98it/s]\n",
      "Test loss 1.278, Test ppl 3.590.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Testing\n",
      "Calculating loss...: 100%|████████████████████| 250/250 [01:23<00:00,  3.01it/s]\n",
      "Test loss 1.278, Test ppl 3.590.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Testing\n",
      "Calculating loss...: 100%|████████████████████| 250/250 [01:23<00:00,  2.99it/s]\n",
      "Test loss 1.278, Test ppl 3.590.\n"
     ]
    }
   ],
   "source": [
    "!uv run python -m mlx_lm lora \\\n",
    "  --model models/qwen-0.5b \\\n",
    "  --resume-adapter-file adapters/nsmc-qwen0p5b-r8/0001600_adapters.safetensors \\\n",
    "  --adapter-path adapters/nsmc-qwen0p5b-r8 \\\n",
    "  --test \\\n",
    "  --data data/lora \\\n",
    "  --test-batches 250\n",
    "  \n",
    "!uv run python -m mlx_lm lora \\\n",
    "  --model models/qwen-0.5b \\\n",
    "  --adapter-path adapters/nsmc-qwen0p5b-r8 \\\n",
    "  --resume-adapter-file adapters/nsmc-qwen0p5b-r8/0002000_adapters.safetensors \\\n",
    "  --test \\\n",
    "  --data data/lora \\\n",
    "  --test-batches 250\n",
    "  \n",
    "!uv run python -m mlx_lm lora \\\n",
    "  --model models/qwen-0.5b \\\n",
    "  --adapter-path adapters/nsmc-qwen0p5b-r8 \\\n",
    "  --resume-adapter-file adapters/nsmc-qwen0p5b-r8/0002400_adapters.safetensors \\\n",
    "  --test \\\n",
    "  --data data/lora \\\n",
    "  --test-batches 250"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week01-model-finetune (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
